{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST is a dataset of Zalando's fashion article images —consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label.\n",
    "\n",
    "“Fashion MNIST” dataset here is used for experimenting and practicing machine learning. Fashion-MNIST dataset is a collection of articles images provided by Zalando(https://zalando.com/) . Thanks to Zalando Research(https://github.com/zalandoresearch/fashion-mnist) for hosting the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "filePath_train_set = 'train-images-idx3-ubyte.gz'\n",
    "filePath_train_label = 'train-labels-idx1-ubyte.gz'\n",
    "\n",
    "filePath_test_set = 't10k-images-idx3-ubyte.gz'\n",
    "filePath_test_label = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "with gzip.open(filePath_train_label, 'rb') as trainLbpath:\n",
    "     trainLabel = np.frombuffer(trainLbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "with gzip.open(filePath_train_set, 'rb') as trainSetpath:\n",
    "     trainSet = np.frombuffer(trainSetpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(trainLabel), 784)\n",
    "\n",
    "with gzip.open(filePath_test_label, 'rb') as testLbpath:\n",
    "     testLabel = np.frombuffer(testLbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "with gzip.open(filePath_test_set, 'rb') as testSetpath:\n",
    "     testSet = np.frombuffer(testSetpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(testLabel), 784)\n",
    "\n",
    "print(type(testLabel))\n",
    "\n",
    "print(trainSet.shape)\n",
    "\n",
    "print(trainLabel.shape)\n",
    "\n",
    "print(testSet.shape)\n",
    "\n",
    "print(testLabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = trainSet, testSet, trainLabel, testLabel\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The class labels for Fashion MNIST are:\n",
    "<br>\n",
    "<br>\n",
    "<b>Label</b> &nbsp;  <b>Description</b>\n",
    "<br>\n",
    "0   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   T-shirt/top\n",
    "<br>\n",
    "1   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Trouser\n",
    "<br>\n",
    "2   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Pullover\n",
    "<br>\n",
    "3   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Dress\n",
    "<br>\n",
    "4   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Coat\n",
    "<br>\n",
    "5   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Sandal\n",
    "<br>\n",
    "6   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Shirt\n",
    "<br>\n",
    "7   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Sneaker\n",
    "<br>\n",
    "8   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Bag\n",
    "<br>\n",
    "9   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Ankle boot\n",
    "<br>\n",
    "\n",
    "Our dataset consists of 60,000 images and each image has 784 features. An image consists of 28x28 pixels, and each pixel is a value from 0 to 255 describing the pixel intensity. 0 for white and 255 for black.\n",
    "\n",
    "Let us have a look at one instance (an article image) of this training dataset X_train.\n",
    "\n",
    "To view a single instance(an article image),all we need to do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib’s imshow() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showImage(data):\n",
    "    some_article = data\n",
    "    some_article_image = some_article.reshape(28, 28) # Reshaping it to get the 28x28 pixels\n",
    "    plt.imshow(some_article_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Shuffling the training dataset - to get uniform samples for cross validation </b>\n",
    "<br>\n",
    "<br>\n",
    "We need to <b>shuffle</b> our training data <b>to ensure that we don't miss out any digit in a cross validation fold</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)   # if you want reproducible results set the random seed value.\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After shuffling, let us see what is the image at X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature Scaling </b>\n",
    "<br>\n",
    "<br>\n",
    "Each image (instance) in the dataset has 784 pixels (features) and <b>value of each feature(pixel) ranges from 0 to 255, and this range is too wide </b>, hence we would need to use <b>feature scaling</b> here to apply <b>standardization</b> to this dataset X_train, so that all the values of each feature (pixel) is in a small range (based on the standard deviation value).\n",
    "<br>\n",
    "<br>\n",
    "<b>  x_scaled = (x - x_mean) / standard deviation    </b>\n",
    "<br>\n",
    "<br>\n",
    "    <b>Scaling is not needed for <i>Decision Tree</i> and <i>Random Forest</i> algorithms</b>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import some libraries that we will use quite often\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we have multiple classes (unique values - 0,1,2,...9) for the <i>label</i> in the target dataset y_train, its a <b>'multi-class' Classification problem</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are basically two strategies using which we can use multiple binary classifiers for multiclass classification. They are\n",
    "\n",
    "    (1) One-versus-all (OvA) strategy also called one-versus-the-rest\n",
    "\n",
    "    (2) One-versus-one (OvO) strategy\n",
    "\n",
    "In OvA strategy, to create a system that can classify the digit images into 10 classes (from 0 to 9), we train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). Then when we want to classify an image, we get the decision score from each classifier for that image and we select the class whose classifier outputs the highest score.\n",
    "\n",
    "\n",
    "<b>We will use OvA strategy</b> for this multi-class Classification problem. When you try to use a 'Binary Classification Algorithm/Model(binary classifier)' for a multi-class classification problem using Scikit Learn, Scikit Learn by default uses OvA strategy internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try <b>SGDClassifier</b> first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.6/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# Let us train the model\n",
    "sgd_clf = SGDClassifier(random_state=42) \n",
    "sgd_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict some instance from the dataset using the above trained model\n",
    "y_train_predict = sgd_clf.predict(X_train[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Scikit-Learn actually trained <b>10 binary classifiers</b>, got their decision scores for the image, and selected the class with the <b>highest score</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the image at X_train[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict all instances of training dataset X_train_scaled using the above trained model\n",
    "y_train_predict = sgd_clf.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy:  0.8489\n",
      "SGD Precision:  0.8493628950770987\n",
      "SGD Recall:  0.8493628950770987\n",
      "SGD F1 Score:  0.8487538349065629\n"
     ]
    }
   ],
   "source": [
    "sgd_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "sgd_precision = precision_score(y_train, y_train_predict, average='weighted')\n",
    "sgd_recall = recall_score(y_train, y_train_predict, average='weighted')\n",
    "sgd_f1_score = f1_score(y_train, y_train_predict, average='weighted')\n",
    "\n",
    "\n",
    "print(\"SGD Accuracy: \", sgd_accuracy)\n",
    "print(\"SGD Precision: \", sgd_precision)\n",
    "print(\"SGD Recall: \", sgd_precision)\n",
    "print(\"SGD F1 Score: \", sgd_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try <b>LogisticRegression</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is <b>multi-class problem</b> (we need to predict multiple classes (0,1,2...9) for the given label), <b>hence we will use Softmax Regression</b>, which is nothing but Logistic Regression for \n",
    "multi-class classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Softmax Regression (multi-class classification problem)\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
    "# 'C' is hyprparameter for regularizing L2\n",
    "# 'lbfgs' is Byoden-Fletcher-Goldfarb-Shanno(BFGS) algorithm\n",
    "log_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict some instance from the dataset using the above trained model\n",
    "y_train_predict = log_clf.predict(X_train[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict all instances of training dataset X_train_scaled using the above trained model\n",
    "y_train_predict = log_clf.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Accuracy:  0.8775333333333334\n",
      "Logistic Precision:  0.876648632278309\n",
      "Logistic Recall:  0.876648632278309\n",
      "Logistic F1 Score:  0.8769281105807729\n"
     ]
    }
   ],
   "source": [
    "log_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "log_precision = precision_score(y_train, y_train_predict, average='weighted')\n",
    "log_recall = recall_score(y_train, y_train_predict, average='weighted')\n",
    "log_f1_score = f1_score(y_train, y_train_predict, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Logistic Accuracy: \", log_accuracy)\n",
    "print(\"Logistic Precision: \", log_precision)\n",
    "print(\"Logistic Recall: \", log_precision)\n",
    "print(\"Logistic F1 Score: \", log_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try <b>DecisionTreeClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_clf = DecisionTreeClassifier(max_depth=50, random_state=42)\n",
    "# Scaling is not needed for Decision Tree algorithm and hence for Random Forest and XGBoost algorithms as they \n",
    "# are also based on Decision Trees. Hence, not using scaled training dataset here\n",
    "dec_tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict some instance from the dataset using the above trained model\n",
    "y_train_predict = dec_tree_clf.predict(X_train[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict all instances of training dataset X_train using the above trained model\n",
    "y_train_predict = dec_tree_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "dec_tree_precision = precision_score(y_train, y_train_predict, average='weighted')\n",
    "dec_tree_recall = recall_score(y_train, y_train_predict, average='weighted')\n",
    "dec_tree_f1_score = f1_score(y_train, y_train_predict, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Decision Tree Accuracy: \", dec_tree_accuracy)\n",
    "print(\"Decision Tree Precision: \", dec_tree_precision)\n",
    "print(\"Decision Tree Recall: \", dec_tree_precision)\n",
    "print(\"Decision Tree F1 Score: \", dec_tree_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try <b>RandomForestClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)\n",
    "# Scaling is not needed for Decision Tree algorithm and hence for Random Forest and XGBoost algorithms as they \n",
    "# are also based on Decision Trees. Hence, not using scaled training dataset here\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict some instance from the data set using the above trained model\n",
    "y_train_predict = rnd_clf.predict(X_train[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict all instances of training dataset X_train using the above trained model\n",
    "y_train_predict = rnd_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "rnd_precision = precision_score(y_train, y_train_predict, average='weighted')\n",
    "rnd_recall = recall_score(y_train, y_train_predict, average='weighted')\n",
    "rnd_f1_score = f1_score(y_train, y_train_predict, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Random Forest Accuracy: \", rnd_accuracy)\n",
    "print(\"Random Forest Precision: \", rnd_precision)\n",
    "print(\"Random Forest Recall: \", rnd_precision)\n",
    "print(\"Random Forest F1 Score: \", rnd_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try <b>Ensemble</b> with <b>soft voting</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "log_clf_ens = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
    "rnd_clf_ens = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf_ens), ('rf', rnd_clf_ens)],\n",
    "    voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict some instance from the data set using the above trained model\n",
    "y_train_predict = voting_clf.predict(X_train[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict all instances of training dataset X_train_scaled using the above trained model\n",
    "y_train_predict = voting_clf.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "voting_precision = precision_score(y_train, y_train_predict, average='weighted')\n",
    "voting_recall = recall_score(y_train, y_train_predict, average='weighted')\n",
    "voting_f1_score = f1_score(y_train, y_train_predict, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Ensemble Accuracy: \", voting_accuracy)\n",
    "print(\"Ensemble Precision: \", voting_precision)\n",
    "print(\"Ensemble Recall: \", voting_precision)\n",
    "print(\"Ensemble F1 Score: \", voting_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try <b>XGBClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=20, max_depth=10, random_state=42)\n",
    "# Scaling is not needed for Decision Tree algorithm and hence for Random Forest and XGBoost algorithms as they \n",
    "# are also based on Decision Trees. Hence, not using scaled training data set here\n",
    "# For max_depth parameter, typical values are between 3 to 10. We have chosen 10\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict some instance from the data set using the above trained model\n",
    "y_train_predict = xgb_clf.predict(X_train[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us predict all instances of training dataset X_train using the above trained model\n",
    "y_train_predict = xgb_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "xgb_precision = precision_score(y_train, y_train_predict, average='weighted')\n",
    "xgb_recall = recall_score(y_train, y_train_predict, average='weighted')\n",
    "xgb_f1_score = f1_score(y_train, y_train_predict, average='weighted')\n",
    "\n",
    "\n",
    "print(\"XGBoost Accuracy: \", xgb_accuracy)\n",
    "print(\"XGBoost Precision: \", xgb_precision)\n",
    "print(\"XGBoost Recall: \", xgb_precision)\n",
    "print(\"XGBoost F1 Score: \", xgb_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Calculating Performance Measures like - Accuracy, Precision, Recall, etc.</b>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us use <b>cross validation</b> to find the proper score of each model, also to ensure that the model is not <i>overfitting</i> or <i>underfitting</i>. \n",
    "<br>\n",
    "<br>\n",
    "<b>NOTE:</b>\n",
    "<br>\n",
    "<b>If the cross validation score values for a performance measure (say accuracy) are not varying significantly for various folds (k-folds) then</b> we can say that the <b>model is not overfitting</b>.\n",
    "<br>\n",
    "<b>If the cross validation score values for a performance measure (say accuracy) are not very low for various folds (k-folds) then</b> we can say that the <b>model is not underfitting</b>.\n",
    "<br>\n",
    "<br>\n",
    "We will perform k-fold cross-validation\n",
    "<br>\n",
    "Will randomly split the training set into 3 distinct subsets called folds (cv=3). Since cross validation is \n",
    "a computing intensive and time consuming process, we are limiting 'cv' (no. of folds) to 3 instead of normally 10 folds.\n",
    "<br>\n",
    "Then will train and evaluate each model 3 times by picking a different fold \n",
    "for evaluation every time and training on the other 2 folds\n",
    "<br>\n",
    "The result will be an array containing the 3 evaluation scores for each of the measures - accuracy, precision, F1 score.\n",
    "<br>\n",
    "We will use <b>cross_val_score()</b> function to calculate <b>accuracy</b>\n",
    "<br>\n",
    " <b>But accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets.</b>\n",
    " <br>\n",
    "A dataset is said to be skewed when some classes are much more frequent than others.\n",
    "<br>\n",
    "Even if the current training dataset may not be skewed, the future test dataset (live) on which the model runs\n",
    "can be skewed, hence, <b>considering we may get skewed dataset in future, let us calculate Precision, Recall and F1 score also for the models.</b>\n",
    "<br>\n",
    "And will use <b>cross_val_predict()</b> function to create <b>confusion matrix</b> to calculate <b>Precision, Recall and F1 score</b>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate mean and standard deviation of each score (e.g. accuracy, precision, etc.)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us <b>calculate accuracy, precision, recall, F1 score for SGDClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_scores = cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\") \n",
    "display_scores(sgd_scores)\n",
    "sgd_accuracy = sgd_scores.mean()\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "confusion_matrix(y_train, y_train_pred)\n",
    "sgd_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "sgd_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "sgd_f1_score = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "print(\"SGD CV Accuracy: \", sgd_accuracy)\n",
    "print(\"SGD CV Precision: \", sgd_precision)\n",
    "print(\"SGD CV Recall: \", sgd_precision)\n",
    "print(\"SGD CV F1 Score: \", sgd_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, <b>SGDClassifier gives accuracy of 83.35% (standrad deviation = 0.0020), precision, recall and F1 score of 83.19%</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us <b>calculate accuracy, precision, recall, F1 Score for Softmax Regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scores = cross_val_score(log_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\") \n",
    "display_scores(log_scores)\n",
    "log_accuracy = log_scores.mean()\n",
    "\n",
    "y_train_pred = cross_val_predict(log_clf, X_train_scaled, y_train, cv=3)\n",
    "confusion_matrix(y_train, y_train_pred)\n",
    "log_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "log_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "log_f1_score = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "print(\"Logistic CV Accuracy: \", log_accuracy)\n",
    "print(\"Logistic CV Precision: \", log_precision)\n",
    "print(\"Logistic CV Recall: \", log_precision)\n",
    "print(\"Logistic CV F1 Score: \", log_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, <b>Softmax Regression</b> (with parameters - multi_class=\"multinomial\", solver=\"lbfgs\" and C=10) <b>gives accuracy of 84.70% (standrad deviation = 0.0022), precision, recall and F1 score of 84.58%</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us <b>calculate accuracy, precision, recall, F1 Score for DecisionTreeClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Features not required for Decision Tree\n",
    "dec_tree_scores = cross_val_score(dec_tree_clf, X_train, y_train, cv=3, scoring=\"accuracy\") \n",
    "display_scores(dec_tree_scores)\n",
    "dec_tree_accuracy = dec_tree_scores.mean()\n",
    "\n",
    "y_train_pred = cross_val_predict(dec_tree_clf, X_train, y_train, cv=3)\n",
    "confusion_matrix(y_train, y_train_pred)\n",
    "dec_tree_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "dec_tree_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "dec_tree_f1_score = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "print(\"Decision Tree CV Accuracy: \", dec_tree_accuracy)\n",
    "print(\"Decision Tree CV Precision: \", dec_tree_precision)\n",
    "print(\"Decision Tree CV Recall: \", dec_tree_precision)\n",
    "print(\"Decision Tree CV F1 Score: \", dec_tree_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, <b>Decision Tree Classifier</b> (with parameter - max_depth=50) <b>gives accuracy of 78.94% (standrad deviation = 0.0016), precision, recall and F1 score of 78.94%</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us <b>calculate accuracy, precision, recall, F1 score for RandomForestClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled features not required for Random Forest (as it is based on Decision Trees)\n",
    "\n",
    "def calculateRandomForestScores():\n",
    "    rnd_scores = cross_val_score(rnd_clf, X_train, y_train, cv=3, scoring=\"accuracy\") \n",
    "    display_scores(rnd_scores)\n",
    "    rnd_accuracy = rnd_scores.mean()\n",
    "\n",
    "    y_train_pred = cross_val_predict(rnd_clf, X_train, y_train, cv=3)\n",
    "    confusion_matrix(y_train, y_train_pred)\n",
    "    rnd_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    rnd_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    rnd_f1_score = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    print(\"Random Forest CV Accuracy: \", rnd_accuracy)\n",
    "    print(\"Random Forest CV Precision: \", rnd_precision)\n",
    "    print(\"Random Forest CV Recall: \", rnd_precision)\n",
    "    print(\"Random Forest CV F1 Score: \", rnd_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateRandomForestScores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, <b>Random Forest Classifier</b> (with parameters - no. of estimators=100 and max_depth=50) <b>gives accuracy of 88.05% (standard deviation = 0.0023), precision, recall and F1 score of 87.95%</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us <b>calculate accuracy, precision, recall, F1 score for Ensemble (Voting Classifier)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateEnsembleScores():\n",
    "    voting_scores = cross_val_score(voting_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\") \n",
    "    display_scores(voting_scores)\n",
    "    voting_accuracy = voting_scores.mean()\n",
    "\n",
    "    y_train_pred = cross_val_predict(voting_clf, X_train_scaled, y_train, cv=3)\n",
    "    confusion_matrix(y_train, y_train_pred)\n",
    "    voting_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    voting_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    voting_f1_score = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    print(\"Ensemble CV Accuracy: \", voting_accuracy)\n",
    "    print(\"Ensemble CV Precision: \", voting_precision)\n",
    "    print(\"Ensemble CV Recall: \", voting_precision)\n",
    "    print(\"Ensemble CV F1 Score: \", voting_f1_score)\n",
    "    \n",
    "calculateEnsembleScores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, <b>Ensemble</b> (of Softmax Regression and Random Forest) <b>with soft voting and no. of estimators as 100 and max_depth as 50</b>, we are getting <b>accuracy of 87.14%, and precision, recall and F1 score of 87%</b>.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Let us <b>try Ensemble with lesser no. of estimators and max_depth (no. of estimators = 20, max_depth=10)</b>, and see the results\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "log_clf_ens = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
    "rnd_clf_ens = RandomForestClassifier(n_estimators=20, max_depth=10, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf_ens), ('rf', rnd_clf_ens)],\n",
    "    voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateEnsembleScores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, <b>Ensemble</b> (of Softmax Regression and Random Forest with soft voting) <b>with no. of estimators as 20 and max_depth as 10</b> gives <b>accuracy of 86.36% (standrad deviation = 0.0026), precision, recall and F1 score of 86.19%</b>. \n",
    "<br>\n",
    "<br>\n",
    "Earlier, for the same <b>Ensemble with no. of estimators as 100 and max_depth as 50</b> we got <b>accuracy of 87.14% (standard deviation = 0.0026), and precision, recall and F1 score of 87%</b>. \n",
    "<br>\n",
    "<br>\n",
    "We see that, <b>for the same Ensemble, by increasing the no. of estimators and max_depth, we are getting better scores</b>. \n",
    "<br>\n",
    "<br>\n",
    "<b>Hence, probably, by adding some more algorithms (models) to the Ensemble, and by trying tuning a few more parameter values, we may be able to improve the scores further</b>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Up to this point, we see that, Random Forest has performed better than all other algorithms(including Ensemble) that we used so far</b>.\n",
    "<br>\n",
    "<br>\n",
    "<b>Now, let us compare Random Forest with XGBoost</b>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us <b>calculate accuracy, precision, recall, F1 Score for XGBClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled features not required for XGBoost (as it is based on Decision Trees)\n",
    "xgb_scores = cross_val_score(xgb_clf, X_train, y_train, cv=3, scoring=\"accuracy\") \n",
    "display_scores(xgb_scores)\n",
    "xgb_accuracy = xgb_scores.mean()\n",
    "\n",
    "y_train_pred = cross_val_predict(xgb_clf, X_train, y_train, cv=3)\n",
    "confusion_matrix(y_train, y_train_pred)\n",
    "xgb_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "xgb_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "xgb_f1_score = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "print(\"XGBoost CV Accuracy: \", xgb_accuracy)\n",
    "print(\"XGBoost CV Precision: \", xgb_precision)\n",
    "print(\"XGBoost CV Recall: \", xgb_precision)\n",
    "print(\"XGBoost CV F1 Score: \", xgb_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, <b>XGBoost Classifier</b> (with parameters - no. of estimators=20 and max_depth=10) <b>gives accuracy of 87.62% (standard deviation = 0.00063), precision, recall and F1 score of 87.53%</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, <b>let us compare the XGBoost scores with that of Random Forest for the same set of parameter values (no. of estimators=20 and max_depth=10)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=20, max_depth=10, random_state=42)\n",
    "# Scaling is not needed for Decision Tree algorithm and hence for Random Forest and XGBoost algorithms as they \n",
    "# are also based on Decision Trees. Hence, not using scaled training dataset here\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateRandomForestScores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for the same set of parameter values (n_estimators=20, max_depth=10), <b>scores of XGBoost are better than that of Random Forest</b>.\n",
    "<br>\n",
    "<br>\n",
    "<b>Random Forest:</b>\n",
    "<br>\n",
    "<b>Accuracy: 84.82 </b>\n",
    "<br>\n",
    "Standard Deviation: 0.0024\n",
    "<br>\n",
    "<b>Precision, Recall, F1 Score: 84.82</b>\n",
    "<br>  \n",
    "<br>\n",
    "<b>XGBoost:</b>\n",
    "<br>\n",
    "<b>Accuracy: 87.62 </b>\n",
    "<br>\n",
    "Standard Deviation: 0.00063\n",
    "<br>\n",
    "<b>Precision, Recall, F1 Score: 87.53 </b>\n",
    "<br>  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Finally, we conclude that XGBoost performance is the best for this problem, hence, we select XGBoost as our final model and will proceed with fine-tuning the same</b>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search takes a lot of time on large datasets. Let us <b>apply Dimensionality Reduction</b> to the training dataset to reduce the number of features in the dataset, <b> so that the time taken for grid search and prediction is reduced</b>. Also, we will calculate the scores based on the <i>reduced</i> features.\n",
    "<br>\n",
    "<br>\n",
    "<b>Let us see, if dimensionality reduction leads to any significant loss of information from the images in our training dataset.</b>\n",
    "<br>\n",
    "<br>\n",
    "If we get a significant loss of information with dimensionality reduction, we will not use dimensionality reduction for our training dataset (and hence the problem).\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our dataset is not like a swiss-roll, therefore, we don't need to convert a 3-dimensional dataset to 2-dimensional plane, etc. \n",
    "<br>\n",
    "Hence, we won't be using <i>Manifold</i> technique for dimensionality reduction here.\n",
    "<br>\n",
    "<br>\n",
    "We will be using <b><i>Projection</i> technique for dimensionality reduction</b> for our problem.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Scikit Learn's PCA class which uses SVD \n",
    "# (Singular Value Decomposition) internally and also the projection\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# with n_components=0.95, in the reduced dataset (X_train_reduced) we got only 187 features (out of original 784)\n",
    "# , and there was significant loss of information (quality) in the 'recovered' (decompressed) images.\n",
    "# Hence, I have selected n_components=0.99, which gives 459 features (out of original 784) \n",
    "# and there is no significant loss of information (quality) in the 'recovered' images \n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if hit your 99% minimum?\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X_train_reduced dataset that we got after applying dimensionality reduction is called <i>compressed dataset</i> (X_train_reduced).\n",
    "<br>\n",
    "Now, to check, if there was any significant information (data) loss for each image of X_train_reduced (that we got after dimensionality reduction), due to compression, as compared to original dataset X_train, let us recover one of the images from X_train_reduced dataset by decompressing (applying <i>inverse</i>) it and compare this <i>recovered</i> image (decompressed image) with its corresponding <i>original</i> image (from X_train).\n",
    "<br>\n",
    "<br>\n",
    "Let us recover (decompress) one of the images (instance) of X_train_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use inverse_transform to decompress back to 784 dimensions\n",
    "\n",
    "X_train_recovered = pca.inverse_transform(X_train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "# Plotting 'original' image\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "# Plotting the corresponding 'recovered' image\n",
    "plot_digits(X_train_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Fine-tuning the selected XGBoost classifier model</b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>By applying dimensionality reduction (with variance ratio of 0.99 i.e. n_components=0.99), we don't get any significant loss of information(quality) in the resulting X_train_reduced dataset. Hence, we will use the X_train_reduced (dimensionally reduced dataset) for grid search</b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try (1x3)=3 combinations of hyperparameters\n",
    "    {'n_estimators': [20], 'max_depth': [8, 10, 12]},\n",
    "    \n",
    "]\n",
    "\n",
    "xgb_clf_grid_search = XGBClassifier(random_state=42)\n",
    "# train across 3 folds, that's a total of 3x3=9 rounds of training \n",
    "grid_search = GridSearchCV(xgb_clf_grid_search, param_grid, cv=3,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameter combinations\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the score of each hyperparameter combination used during the grid search\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Now, let us <b>evaluate our selected XGBoost model, using best parameters, on the test dataset</b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>If you don't get significant loss of information by applying dimensionality reduction on training dataset, then apply dimensionality reduction on your test dataset(X_test) to get X_test_reduced dataset (dimensionally reduced dataset) and use the X_test_reduced (dimensionally reduced dataset) for evaluating the model on test dataset (X_test_reduced), else, use original test dataset X_test for evaluating the model on test dataset </b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on the test Set\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Remember, you have to use pca object of training dataset (you got on training dataset during dimensionality reduction)\n",
    "# and only apply transform on test dataset (not fit_transform) - highly important\n",
    "\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "\n",
    "y_test_predict = final_model.predict(X_test_reduced)\n",
    "\n",
    "\n",
    "confusion_matrix(y_test, y_test_predict)\n",
    "final_accuracy = accuracy_score(y_test, y_test_predict)\n",
    "final_precision = precision_score(y_test, y_test_predict, average='weighted')\n",
    "final_recall = recall_score(y_test, y_test_predict, average='weighted')\n",
    "final_f1_score = f1_score(y_test, y_test_predict, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Final Accuracy: \", final_accuracy)\n",
    "print(\"Final Precision: \", final_precision)\n",
    "print(\"Final Recall: \", final_precision)\n",
    "print(\"Final F1 Score: \", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
